{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a9f22a",
   "metadata": {},
   "source": [
    "# Split Conformal Prediction API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc48463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "class SplitConformalPrediction:\n",
    "    def __init__(self, data, W, Y, X, test_size1=0.4, test_size2=0.5, lower_quantile=0.05, upper_quantile=0.95, random_state=42):\n",
    "        self.data = data\n",
    "        self.W = W\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.test_size1 = test_size1\n",
    "        self.test_size2 = test_size2\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.split_data()\n",
    "        self.train_nn_Y0_Y1()\n",
    "        self.train_nn_ITE()\n",
    "        self.train_quantile_models()\n",
    "    \n",
    "    def split_data(self):\n",
    "        # Split the original data into prediction_model_train and temp_data\n",
    "        self.prediction_model_train, temp_data = train_test_split(\n",
    "            self.data, \n",
    "            test_size=self.test_size1, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Further split temp_data into quantile_model_train and quantile_model_calibration\n",
    "        self.quantile_model_train, self.quantile_model_calibration = train_test_split(\n",
    "            temp_data, \n",
    "            test_size=self.test_size2, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "    def train_nn_Y0_Y1(self):\n",
    "        # Prepare data for training Y0 and Y1 models\n",
    "        train_data_Y0 = self.prediction_model_train[self.prediction_model_train[self.W] == 0]\n",
    "        X_train_Y0 = train_data_Y0[self.X]\n",
    "        y_train_Y0 = train_data_Y0[self.Y]\n",
    "\n",
    "        train_data_Y1 = self.prediction_model_train[self.prediction_model_train[self.W] == 1]\n",
    "        X_train_Y1 = train_data_Y1[self.X]\n",
    "        y_train_Y1 = train_data_Y1[self.Y]\n",
    "\n",
    "        # Train model for Y0\n",
    "        self.model_Y0 = MLPRegressor(random_state=self.random_state)\n",
    "        self.model_Y0.fit(X_train_Y0, y_train_Y0)\n",
    "\n",
    "        # Train model for Y1\n",
    "        self.model_Y1 = MLPRegressor(random_state=self.random_state)\n",
    "        self.model_Y1.fit(X_train_Y1, y_train_Y1)\n",
    "\n",
    "    def train_nn_ITE(self):\n",
    "        # Prepare pseudo ITE data\n",
    "        pseudo_ITE_data = self.prediction_model_train.copy()\n",
    "        pseudo_ITE_data['Y1'] = np.where(pseudo_ITE_data[self.W] == 1, pseudo_ITE_data[self.Y], self.model_Y0.predict(pseudo_ITE_data[self.X]))\n",
    "        pseudo_ITE_data['Y0'] = np.where(pseudo_ITE_data[self.W] == 0, pseudo_ITE_data[self.Y], self.model_Y1.predict(pseudo_ITE_data[self.X]))\n",
    "        pseudo_ITE_data['ITE'] = pseudo_ITE_data['Y1'] - pseudo_ITE_data['Y0']\n",
    "\n",
    "        # Train model for ITE\n",
    "        X_ITE = pseudo_ITE_data[self.X]\n",
    "        y_ITE = pseudo_ITE_data['ITE']\n",
    "        self.model_ITE = MLPRegressor(random_state=self.random_state)\n",
    "        self.model_ITE.fit(X_ITE, y_ITE)\n",
    "\n",
    "    def train_quantile_models(self):\n",
    "        # Train lower bound quantile model\n",
    "        self.lower_quantile_model = GradientBoostingRegressor(loss='quantile', alpha=self.lower_quantile, random_state=self.random_state)\n",
    "        self.lower_quantile_model.fit(self.quantile_model_train[self.X], self.quantile_model_train[self.Y])\n",
    "\n",
    "        # Train upper bound quantile model\n",
    "        self.upper_quantile_model = GradientBoostingRegressor(loss='quantile', alpha=self.upper_quantile, random_state=self.random_state)\n",
    "        self.upper_quantile_model.fit(self.quantile_model_train[self.X], self.quantile_model_train[self.Y])\n",
    "\n",
    "    def calculate_calibrated_interval(self):\n",
    "        # Calculate predicted ITE for the calibration set\n",
    "        X_calibration = self.quantile_model_calibration[self.X]\n",
    "        actual_ite_calibration = self.quantile_model_calibration[self.Y]\n",
    "        pred_ite_calibration = self.model_ITE.predict(X_calibration)\n",
    "\n",
    "        # Calculate prediction errors and sort them\n",
    "        prediction_errors = np.abs(actual_ite_calibration - pred_ite_calibration)\n",
    "        sorted_errors = np.sort(prediction_errors)\n",
    "\n",
    "        # Calculate calibrated bounds\n",
    "        lower_error = sorted_errors[int(len(sorted_errors) * self.lower_quantile)]\n",
    "        upper_error = sorted_errors[int(len(sorted_errors) * self.upper_quantile)]\n",
    "        calibrated_lower_bound = pred_ite_calibration - lower_error\n",
    "        calibrated_upper_bound = pred_ite_calibration + upper_error\n",
    "\n",
    "        # Calculate coverage rate\n",
    "        calibrated_coverage_rate = np.mean((actual_ite_calibration >= calibrated_lower_bound) & (actual_ite_calibration <= calibrated_upper_bound))\n",
    "\n",
    "        return calibrated_lower_bound, calibrated_upper_bound, calibrated_coverage_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f26c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the class with the data and parameters\n",
    "W = \"treatment\"\n",
    "Y = \"wage_2010\"\n",
    "X = [col for col in data.columns if col not in [W, Y]]\n",
    "scp = SplitConformalPrediction(data, W=W, Y=Y, X=X, random_state=121)\n",
    "\n",
    "# Calculate the calibrated confidence interval\n",
    "calibrated_lower_bound, calibrated_upper_bound, calibrated_coverage_rate = scp.calculate_calibrated_interval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_coverage_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1d7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1c520e",
   "metadata": {},
   "source": [
    "# Step-by-step Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2ddee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours</th>\n",
       "      <th>female</th>\n",
       "      <th>IQ</th>\n",
       "      <th>KWW</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>south</th>\n",
       "      <th>urban</th>\n",
       "      <th>sibs</th>\n",
       "      <th>brthord</th>\n",
       "      <th>meduc</th>\n",
       "      <th>feduc</th>\n",
       "      <th>wage_2005</th>\n",
       "      <th>emp_2005</th>\n",
       "      <th>exper_2005</th>\n",
       "      <th>tenure_2005</th>\n",
       "      <th>age_2005</th>\n",
       "      <th>married_2005</th>\n",
       "      <th>treatment</th>\n",
       "      <th>wage_2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>736.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.691576</td>\n",
       "      <td>0.490489</td>\n",
       "      <td>102.471467</td>\n",
       "      <td>36.229620</td>\n",
       "      <td>13.705163</td>\n",
       "      <td>0.080163</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.720109</td>\n",
       "      <td>2.820652</td>\n",
       "      <td>2.183424</td>\n",
       "      <td>10.808424</td>\n",
       "      <td>10.305707</td>\n",
       "      <td>779.342052</td>\n",
       "      <td>0.900815</td>\n",
       "      <td>11.402174</td>\n",
       "      <td>7.199728</td>\n",
       "      <td>32.972826</td>\n",
       "      <td>0.899457</td>\n",
       "      <td>0.341033</td>\n",
       "      <td>1068.358240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.828039</td>\n",
       "      <td>0.500249</td>\n",
       "      <td>14.616429</td>\n",
       "      <td>7.656947</td>\n",
       "      <td>2.233639</td>\n",
       "      <td>0.271730</td>\n",
       "      <td>0.467045</td>\n",
       "      <td>0.449251</td>\n",
       "      <td>2.265484</td>\n",
       "      <td>1.515560</td>\n",
       "      <td>2.827462</td>\n",
       "      <td>3.280994</td>\n",
       "      <td>436.803111</td>\n",
       "      <td>0.299113</td>\n",
       "      <td>4.208666</td>\n",
       "      <td>5.017743</td>\n",
       "      <td>3.047349</td>\n",
       "      <td>0.300928</td>\n",
       "      <td>0.474379</td>\n",
       "      <td>602.048732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>744.485697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>773.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1077.658545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1005.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1437.733830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2668.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3791.589594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hours      female          IQ         KWW        educ       black  \\\n",
       "count  736.000000  736.000000  736.000000  736.000000  736.000000  736.000000   \n",
       "mean    39.691576    0.490489  102.471467   36.229620   13.705163    0.080163   \n",
       "std     14.828039    0.500249   14.616429    7.656947    2.233639    0.271730   \n",
       "min      0.000000    0.000000   54.000000   13.000000    9.000000    0.000000   \n",
       "25%     40.000000    0.000000   94.000000   31.750000   12.000000    0.000000   \n",
       "50%     40.000000    0.000000  104.000000   37.000000   13.000000    0.000000   \n",
       "75%     45.000000    1.000000  113.000000   42.000000   16.000000    0.000000   \n",
       "max     80.000000    1.000000  145.000000   56.000000   18.000000    1.000000   \n",
       "\n",
       "            south       urban        sibs     brthord       meduc       feduc  \\\n",
       "count  736.000000  736.000000  736.000000  736.000000  736.000000  736.000000   \n",
       "mean     0.320652    0.720109    2.820652    2.183424   10.808424   10.305707   \n",
       "std      0.467045    0.449251    2.265484    1.515560    2.827462    3.280994   \n",
       "min      0.000000    0.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    1.000000    1.000000    9.000000    8.000000   \n",
       "50%      0.000000    1.000000    2.000000    2.000000   12.000000   11.000000   \n",
       "75%      1.000000    1.000000    4.000000    3.000000   12.000000   12.000000   \n",
       "max      1.000000    1.000000   14.000000   10.000000   18.000000   18.000000   \n",
       "\n",
       "         wage_2005    emp_2005  exper_2005  tenure_2005    age_2005  \\\n",
       "count   736.000000  736.000000  736.000000   736.000000  736.000000   \n",
       "mean    779.342052    0.900815   11.402174     7.199728   32.972826   \n",
       "std     436.803111    0.299113    4.208666     5.017743    3.047349   \n",
       "min       0.000000    0.000000    1.000000     0.000000   28.000000   \n",
       "25%     525.000000    1.000000    8.000000     3.000000   30.000000   \n",
       "50%     773.625000    1.000000   11.000000     7.000000   33.000000   \n",
       "75%    1005.437500    1.000000   15.000000    11.000000   36.000000   \n",
       "max    2668.000000    1.000000   22.000000    22.000000   38.000000   \n",
       "\n",
       "       married_2005   treatment    wage_2010  \n",
       "count    736.000000  736.000000   736.000000  \n",
       "mean       0.899457    0.341033  1068.358240  \n",
       "std        0.300928    0.474379   602.048732  \n",
       "min        0.000000    0.000000     0.000000  \n",
       "25%        1.000000    0.000000   744.485697  \n",
       "50%        1.000000    0.000000  1077.658545  \n",
       "75%        1.000000    1.000000  1437.733830  \n",
       "max        1.000000    1.000000  3791.589594  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = r\"gender_equality_data_no_na.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show the first few rows of the dataset and its summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Load the data\n",
    "file_path = 'gender_equality_data_no_na.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into three parts\n",
    "prediction_model_train, temp_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "quantile_model_train, quantile_model_calibration = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define features and target variables\n",
    "W = \"treatment\"\n",
    "Y = \"wage_2010\"\n",
    "X = [col for col in data.columns if col not in [W, Y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81cca5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPRegressor(random_state=42), MLPRegressor(random_state=42))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow the receipe of X-learner\n",
    "\n",
    "# Train a simple Neural Network model predicting Y0 (wage_2010 for treatment=0)\n",
    "train_data_Y0 = prediction_model_train[prediction_model_train[W] == 0]\n",
    "X_train_Y0 = train_data_Y0[X]\n",
    "y_train_Y0 = train_data_Y0[Y]\n",
    "model_Y0 = MLPRegressor(random_state=42)\n",
    "model_Y0.fit(X_train_Y0, y_train_Y0)\n",
    "\n",
    "# Train another simple Neural Network model predicting Y1 (wage_2010 for treatment=1)\n",
    "train_data_Y1 = prediction_model_train[prediction_model_train[W] == 1]\n",
    "X_train_Y1 = train_data_Y1[X]\n",
    "y_train_Y1 = train_data_Y1[Y]\n",
    "model_Y1 = MLPRegressor(random_state=42)\n",
    "model_Y1.fit(X_train_Y1, y_train_Y1)\n",
    "\n",
    "model_Y0, model_Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a2cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data representing pseudo ITE (Individual Treatment Effect)\n",
    "pseudo_ITE_data = prediction_model_train.copy()\n",
    "pseudo_ITE_data['Y1'] = None\n",
    "pseudo_ITE_data['Y0'] = None\n",
    "\n",
    "# For rows where W=1, Y1=Y, and Y0 is predicted by model_Y0\n",
    "mask_W1 = pseudo_ITE_data[W] == 1\n",
    "pseudo_ITE_data.loc[mask_W1, 'Y1'] = pseudo_ITE_data.loc[mask_W1, Y]\n",
    "pseudo_ITE_data.loc[mask_W1, 'Y0'] = model_Y0.predict(pseudo_ITE_data.loc[mask_W1, X])\n",
    "\n",
    "# For rows where W=0, Y0=Y, and Y1 is predicted by model_Y1\n",
    "mask_W0 = pseudo_ITE_data[W] == 0\n",
    "pseudo_ITE_data.loc[mask_W0, 'Y0'] = pseudo_ITE_data.loc[mask_W0, Y]\n",
    "pseudo_ITE_data.loc[mask_W0, 'Y1'] = model_Y1.predict(pseudo_ITE_data.loc[mask_W0, X])\n",
    "\n",
    "# Calculate the pseudo ITE: ITE = Y1 - Y0\n",
    "pseudo_ITE_data['ITE'] = pseudo_ITE_data['Y1'] - pseudo_ITE_data['Y0']\n",
    "\n",
    "# Prepare features and target for ITE model\n",
    "X_ITE = pseudo_ITE_data[X]\n",
    "y_ITE = pseudo_ITE_data['ITE']\n",
    "\n",
    "# Fit the ITE to another simple Neural Network\n",
    "model_ITE = MLPRegressor(random_state=42)\n",
    "model_ITE.fit(X_ITE, y_ITE)\n",
    "\n",
    "model_ITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3112cc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([227.98618267, 207.29785914, 252.8251098 , 191.38238592,\n",
       "        183.06390687, 238.41118759, 173.62007684, 235.92821863,\n",
       "        274.24334126, 192.66731149]),\n",
       " array([750.995554548319, 267.0314392492619, 331.6038042017951,\n",
       "        275.92708550816906, 195.4425383669277, 320.14558252573613,\n",
       "        175.23488594360128, 222.96862697266886, 207.448234410542,\n",
       "        127.61528483408733], dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Using model_ITE, predict the ITE for Quantile Model Training Data\n",
    "X_quantile = quantile_model_train[X]\n",
    "pred_ite = model_ITE.predict(X_quantile)\n",
    "\n",
    "# 2. Prepare the actual ITE for Quantile Model Training Data\n",
    "quantile_ITE_data = quantile_model_train.copy()\n",
    "quantile_ITE_data['Y1'] = None\n",
    "quantile_ITE_data['Y0'] = None\n",
    "\n",
    "# For rows where W=1, Y1=Y, and Y0 is predicted by model_Y0\n",
    "mask_W1_quantile = quantile_ITE_data[W] == 1\n",
    "quantile_ITE_data.loc[mask_W1_quantile, 'Y1'] = quantile_ITE_data.loc[mask_W1_quantile, Y]\n",
    "quantile_ITE_data.loc[mask_W1_quantile, 'Y0'] = model_Y0.predict(quantile_ITE_data.loc[mask_W1_quantile, X])\n",
    "\n",
    "# For rows where W=0, Y0=Y, and Y1 is predicted by model_Y1\n",
    "mask_W0_quantile = quantile_ITE_data[W] == 0\n",
    "quantile_ITE_data.loc[mask_W0_quantile, 'Y0'] = quantile_ITE_data.loc[mask_W0_quantile, Y]\n",
    "quantile_ITE_data.loc[mask_W0_quantile, 'Y1'] = model_Y1.predict(quantile_ITE_data.loc[mask_W0_quantile, X])\n",
    "\n",
    "# Calculate the actual ITE: ITE = Y1 - Y0\n",
    "quantile_ITE_data['actual_ite'] = quantile_ITE_data['Y1'] - quantile_ITE_data['Y0']\n",
    "\n",
    "actual_ite = quantile_ITE_data['actual_ite'].values\n",
    "\n",
    "pred_ite[:10], actual_ite[:10]  # Displaying first 10 values of actual_ite for a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4525be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-75.89817172, 106.70172788, 115.8019146 , 118.43842557,\n",
       "         92.91491302, 127.5386123 , 112.6503457 ,  92.61794121,\n",
       "        100.42338973, 126.77992619]),\n",
       " array([750.98325003, 483.31701257, 477.54225273, 395.2192382 ,\n",
       "        339.44517005, 409.23182337, 641.15387069, 339.01792247,\n",
       "        371.49106345, 430.46629698]),\n",
       " 0.9115646258503401)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# 1. Train two quantile models, one for the upper bound and the other for the lower bound\n",
    "# Quantile levels for 90% coverage: lower bound at 5% and upper bound at 95%\n",
    "lower_quantile = 0.05\n",
    "upper_quantile = 0.95\n",
    "\n",
    "# Train the lower bound quantile model\n",
    "lower_quantile_model = GradientBoostingRegressor(loss='quantile', alpha=lower_quantile, random_state=42)\n",
    "lower_quantile_model.fit(X_quantile, actual_ite)\n",
    "\n",
    "# Train the upper bound quantile model\n",
    "upper_quantile_model = GradientBoostingRegressor(loss='quantile', alpha=upper_quantile, random_state=42)\n",
    "upper_quantile_model.fit(X_quantile, actual_ite)\n",
    "\n",
    "# 2. Predict the upper and lower bounds for the Quantile Model Training Data\n",
    "lower_bound = lower_quantile_model.predict(X_quantile)\n",
    "upper_bound = upper_quantile_model.predict(X_quantile)\n",
    "\n",
    "# Calculate the coverage rate\n",
    "coverage_rate = ((pred_ite >= lower_bound) & (pred_ite <= upper_bound)).mean()\n",
    "\n",
    "lower_bound[:10], upper_bound[:10], coverage_rate  # Displaying first 10 values of lower and upper bounds and the coverage rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa924de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b9c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
